{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('nlp_learn.db')\n",
    "\n",
    "# data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);\n",
    "query = \"SELECT id, title FROM books\"\n",
    "result_set = conn.execute(query).fetchall()\n",
    "data = pd.DataFrame(result_set, columns = ['id', 'title'])\n",
    "\n",
    "data_text = data[['title']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671255"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Things Fall Apart</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fairy tales</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Divine Comedy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Epic Of Gilgamesh</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Book Of Job</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>One Thousand and One Nights</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Njál's Saga</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Le Père Goriot</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Molloy, Malone Dies, The Unnamable, the trilogy</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title  index\n",
       "0                                Things Fall Apart      0\n",
       "1                                      Fairy tales      1\n",
       "2                                The Divine Comedy      2\n",
       "3                            The Epic Of Gilgamesh      3\n",
       "4                                  The Book Of Job      4\n",
       "5                      One Thousand and One Nights      5\n",
       "6                                      Njál's Saga      6\n",
       "7                              Pride and Prejudice      7\n",
       "8                                   Le Père Goriot      8\n",
       "9  Molloy, Malone Dies, The Unnamable, the trilogy      9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\salman.akhatar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 1:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Jump', 'Start', 'Your', 'Gluten-Free', 'Diet!', 'Living', 'with', 'Celiac', '/', 'Coeliac', 'Disease', '&', 'Gluten', 'Intolerance']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['jump', 'start', 'gluten', 'free', 'diet', 'live', 'celiac', 'coeliac', 'diseas', 'gluten', 'intoler']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4130].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['title'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   [thing, fall, apart]\n",
       "1                          [fairi, tale]\n",
       "2                        [divin, comedi]\n",
       "3                      [epic, gilgamesh]\n",
       "4                            [book, job]\n",
       "5                      [thousand, night]\n",
       "6                           [njál, saga]\n",
       "7                      [pride, prejudic]\n",
       "8                     [le, père, goriot]\n",
       "9    [molloy, malon, die, unnam, trilog]\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203957"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 apart\n",
      "1 fall\n",
      "2 thing\n",
      "3 fairi\n",
      "4 tale\n",
      "5 comedi\n",
      "6 divin\n",
      "7 epic\n",
      "8 gilgamesh\n",
      "9 book\n",
      "10 job\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(694, 1),\n",
       " (779, 1),\n",
       " (1336, 1),\n",
       " (1443, 1),\n",
       " (1844, 1),\n",
       " (1888, 1),\n",
       " (3103, 2),\n",
       " (4419, 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 694 (\"jump\") appears 1 time.\n",
      "Word 779 (\"live\") appears 1 time.\n",
      "Word 1336 (\"diet\") appears 1 time.\n",
      "Word 1443 (\"start\") appears 1 time.\n",
      "Word 1844 (\"diseas\") appears 1 time.\n",
      "Word 1888 (\"free\") appears 1 time.\n",
      "Word 3103 (\"gluten\") appears 2 time.\n",
      "Word 4419 (\"intoler\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4130]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7196780521769552), (1, 0.47007123578090043), (2, 0.5109760605999079)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LDA using Bag of Words\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-ca1ca7d55477>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# lda_model.save('trained_model/lda_train_bow.model')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'trained_model/lda_train_bow'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LDA Training - Complete\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: write() argument must be str, not bytes"
     ]
    }
   ],
   "source": [
    "print(\"Running LDA using Bag of Words\")\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=25, id2word=dictionary, passes=2, workers=2)\n",
    "lda_model.save('trained_model/lda_train_bow.model')\n",
    "print(\"LDA Training - Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.126*\"world\" + 0.069*\"histori\" + 0.049*\"year\" + 0.025*\"ii\" + 0.024*\"faith\" + 0.023*\"centuri\" + 0.022*\"war\" + 0.019*\"new\" + 0.018*\"ancient\" + 0.018*\"simpl\"\n",
      "Topic: 1 \n",
      "Words: 0.131*\"la\" + 0.060*\"el\" + 0.040*\"del\" + 0.039*\"le\" + 0.030*\"come\" + 0.026*\"wild\" + 0.025*\"break\" + 0.024*\"los\" + 0.019*\"daughter\" + 0.016*\"princess\"\n",
      "Topic: 2 \n",
      "Words: 0.245*\"book\" + 0.087*\"seri\" + 0.022*\"recip\" + 0.021*\"kid\" + 0.020*\"age\" + 0.017*\"queen\" + 0.016*\"child\" + 0.016*\"free\" + 0.012*\"alpha\" + 0.012*\"easi\"\n",
      "Topic: 3 \n",
      "Words: 0.100*\"time\" + 0.057*\"way\" + 0.028*\"garden\" + 0.026*\"stone\" + 0.025*\"kill\" + 0.024*\"old\" + 0.020*\"long\" + 0.020*\"seven\" + 0.017*\"cook\" + 0.017*\"troubl\"\n",
      "Topic: 4 \n",
      "Words: 0.076*\"day\" + 0.055*\"black\" + 0.042*\"citi\" + 0.032*\"guid\" + 0.031*\"end\" + 0.027*\"white\" + 0.022*\"human\" + 0.019*\"build\" + 0.018*\"handbook\" + 0.017*\"color\"\n",
      "Topic: 5 \n",
      "Words: 0.069*\"american\" + 0.052*\"rise\" + 0.046*\"king\" + 0.037*\"saga\" + 0.034*\"soul\" + 0.027*\"summer\" + 0.026*\"land\" + 0.019*\"passion\" + 0.018*\"state\" + 0.015*\"deep\"\n",
      "Topic: 6 \n",
      "Words: 0.042*\"power\" + 0.036*\"thing\" + 0.030*\"les\" + 0.028*\"practic\" + 0.026*\"sweet\" + 0.025*\"anim\" + 0.024*\"word\" + 0.020*\"das\" + 0.020*\"save\" + 0.019*\"self\"\n",
      "Topic: 7 \n",
      "Words: 0.051*\"dead\" + 0.042*\"journey\" + 0.038*\"complet\" + 0.028*\"make\" + 0.023*\"read\" + 0.021*\"surviv\" + 0.020*\"bodi\" + 0.019*\"polit\" + 0.018*\"teach\" + 0.018*\"van\"\n",
      "Topic: 8 \n",
      "Words: 0.052*\"war\" + 0.043*\"christma\" + 0.040*\"boy\" + 0.037*\"dream\" + 0.035*\"dragon\" + 0.034*\"littl\" + 0.032*\"angel\" + 0.031*\"game\" + 0.026*\"scienc\" + 0.025*\"cat\"\n",
      "Topic: 9 \n",
      "Words: 0.082*\"man\" + 0.078*\"girl\" + 0.047*\"home\" + 0.027*\"life\" + 0.027*\"go\" + 0.024*\"bibl\" + 0.022*\"find\" + 0.022*\"studi\" + 0.020*\"real\" + 0.020*\"guid\"\n",
      "Topic: 10 \n",
      "Words: 0.040*\"edit\" + 0.034*\"light\" + 0.030*\"kiss\" + 0.029*\"earth\" + 0.029*\"wolf\" + 0.028*\"club\" + 0.023*\"hell\" + 0.020*\"travel\" + 0.019*\"mountain\" + 0.019*\"west\"\n",
      "Topic: 11 \n",
      "Words: 0.097*\"vol\" + 0.075*\"novel\" + 0.046*\"fall\" + 0.046*\"star\" + 0.044*\"collect\" + 0.038*\"best\" + 0.030*\"blue\" + 0.027*\"friend\" + 0.022*\"master\" + 0.019*\"song\"\n",
      "Topic: 12 \n",
      "Words: 0.066*\"night\" + 0.055*\"death\" + 0.045*\"blood\" + 0.043*\"true\" + 0.037*\"stori\" + 0.029*\"memoir\" + 0.021*\"legend\" + 0.020*\"forev\" + 0.019*\"hunter\" + 0.018*\"mother\"\n",
      "Topic: 13 \n",
      "Words: 0.051*\"trilog\" + 0.036*\"bear\" + 0.034*\"vampir\" + 0.031*\"america\" + 0.023*\"doctor\" + 0.020*\"princ\" + 0.020*\"du\" + 0.019*\"warrior\" + 0.017*\"univers\" + 0.017*\"heal\"\n",
      "Topic: 14 \n",
      "Words: 0.065*\"die\" + 0.059*\"art\" + 0.048*\"der\" + 0.041*\"men\" + 0.034*\"ladi\" + 0.023*\"green\" + 0.022*\"spirit\" + 0.022*\"john\" + 0.021*\"river\" + 0.019*\"texa\"\n",
      "Topic: 15 \n",
      "Words: 0.061*\"adventur\" + 0.036*\"good\" + 0.034*\"chang\" + 0.034*\"moon\" + 0.029*\"play\" + 0.022*\"get\" + 0.021*\"think\" + 0.020*\"case\" + 0.019*\"crime\" + 0.015*\"midnight\"\n",
      "Topic: 16 \n",
      "Words: 0.097*\"mysteri\" + 0.082*\"secret\" + 0.049*\"di\" + 0.046*\"murder\" + 0.040*\"il\" + 0.027*\"school\" + 0.023*\"set\" + 0.021*\"box\" + 0.018*\"lord\" + 0.016*\"justic\"\n",
      "Topic: 17 \n",
      "Words: 0.062*\"famili\" + 0.053*\"great\" + 0.033*\"poem\" + 0.030*\"like\" + 0.027*\"road\" + 0.025*\"design\" + 0.021*\"danc\" + 0.019*\"look\" + 0.018*\"inspir\" + 0.018*\"jack\"\n",
      "Topic: 18 \n",
      "Words: 0.069*\"live\" + 0.045*\"women\" + 0.038*\"work\" + 0.035*\"woman\" + 0.028*\"step\" + 0.026*\"devil\" + 0.026*\"truth\" + 0.024*\"letter\" + 0.024*\"second\" + 0.022*\"classic\"\n",
      "Topic: 19 \n",
      "Words: 0.042*\"write\" + 0.041*\"shadow\" + 0.039*\"en\" + 0.034*\"romanc\" + 0.033*\"red\" + 0.032*\"know\" + 0.029*\"mind\" + 0.025*\"busi\" + 0.025*\"diari\" + 0.022*\"modern\"\n",
      "Topic: 20 \n",
      "Words: 0.107*\"volum\" + 0.098*\"heart\" + 0.040*\"dog\" + 0.036*\"beauti\" + 0.031*\"high\" + 0.029*\"season\" + 0.020*\"cookbook\" + 0.019*\"cross\" + 0.017*\"hand\" + 0.017*\"edg\"\n",
      "Topic: 21 \n",
      "Words: 0.233*\"love\" + 0.059*\"chronicl\" + 0.046*\"magic\" + 0.029*\"witch\" + 0.027*\"eye\" + 0.020*\"tell\" + 0.019*\"winter\" + 0.019*\"water\" + 0.017*\"line\" + 0.017*\"essay\"\n",
      "Topic: 22 \n",
      "Words: 0.044*\"bride\" + 0.028*\"lie\" + 0.027*\"miss\" + 0.026*\"happi\" + 0.024*\"place\" + 0.024*\"sister\" + 0.021*\"knight\" + 0.021*\"english\" + 0.019*\"origin\" + 0.018*\"futur\"\n",
      "Topic: 23 \n",
      "Words: 0.175*\"stori\" + 0.058*\"god\" + 0.041*\"hous\" + 0.036*\"short\" + 0.030*\"babi\" + 0.029*\"big\" + 0.027*\"peopl\" + 0.023*\"في\" + 0.019*\"sex\" + 0.018*\"run\"\n",
      "Topic: 24 \n",
      "Words: 0.097*\"tale\" + 0.084*\"dark\" + 0.050*\"lose\" + 0.036*\"children\" + 0.032*\"ghost\" + 0.028*\"learn\" + 0.025*\"natur\" + 0.024*\"cultur\" + 0.023*\"hero\" + 0.021*\"insid\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LDA using TF-IDF\n",
      "LDA Training - Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Running LDA using TF-IDF\")\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=25, id2word=dictionary, passes=2, workers=4)\n",
    "print(\"LDA Training - Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.054*\"die\" + 0.037*\"der\" + 0.023*\"miss\" + 0.021*\"son\" + 0.018*\"das\" + 0.017*\"من\" + 0.016*\"wish\" + 0.015*\"im\" + 0.015*\"grace\" + 0.015*\"des\"\n",
      "Topic: 1 Word: 0.028*\"road\" + 0.022*\"old\" + 0.022*\"mr\" + 0.019*\"cowboy\" + 0.019*\"demon\" + 0.017*\"father\" + 0.016*\"hors\" + 0.016*\"awaken\" + 0.016*\"affair\" + 0.014*\"north\"\n",
      "Topic: 2 Word: 0.047*\"death\" + 0.034*\"come\" + 0.026*\"go\" + 0.019*\"dont\" + 0.019*\"lord\" + 0.018*\"home\" + 0.018*\"water\" + 0.016*\"travel\" + 0.015*\"let\" + 0.014*\"day\"\n",
      "Topic: 3 Word: 0.038*\"lose\" + 0.035*\"blood\" + 0.033*\"love\" + 0.027*\"ladi\" + 0.020*\"book\" + 0.018*\"walk\" + 0.018*\"sky\" + 0.017*\"time\" + 0.016*\"long\" + 0.014*\"marriag\"\n",
      "Topic: 4 Word: 0.027*\"stori\" + 0.023*\"write\" + 0.022*\"short\" + 0.021*\"daughter\" + 0.018*\"letter\" + 0.017*\"best\" + 0.016*\"fiction\" + 0.015*\"hope\" + 0.014*\"select\" + 0.014*\"scienc\"\n",
      "Topic: 5 Word: 0.052*\"night\" + 0.029*\"men\" + 0.025*\"friend\" + 0.024*\"wolf\" + 0.022*\"los\" + 0.021*\"brother\" + 0.020*\"kill\" + 0.016*\"rock\" + 0.014*\"hunt\" + 0.014*\"wind\"\n",
      "Topic: 6 Word: 0.046*\"dead\" + 0.033*\"game\" + 0.027*\"poem\" + 0.027*\"play\" + 0.027*\"il\" + 0.024*\"eye\" + 0.023*\"sea\" + 0.018*\"van\" + 0.016*\"les\" + 0.015*\"meet\"\n",
      "Topic: 7 Word: 0.036*\"angel\" + 0.029*\"cat\" + 0.022*\"sister\" + 0.018*\"take\" + 0.017*\"return\" + 0.014*\"secret\" + 0.013*\"destini\" + 0.013*\"royal\" + 0.013*\"key\" + 0.012*\"paradis\"\n",
      "Topic: 8 Word: 0.045*\"fall\" + 0.030*\"blue\" + 0.025*\"soul\" + 0.020*\"earth\" + 0.016*\"cross\" + 0.015*\"rule\" + 0.015*\"bodi\" + 0.015*\"memori\" + 0.014*\"edg\" + 0.014*\"real\"\n",
      "Topic: 9 Word: 0.020*\"faith\" + 0.020*\"child\" + 0.020*\"anim\" + 0.019*\"hide\" + 0.014*\"univers\" + 0.014*\"christian\" + 0.013*\"bind\" + 0.013*\"lesson\" + 0.013*\"god\" + 0.012*\"room\"\n",
      "Topic: 10 Word: 0.035*\"boy\" + 0.030*\"know\" + 0.028*\"summer\" + 0.023*\"doctor\" + 0.021*\"tree\" + 0.017*\"need\" + 0.016*\"troubl\" + 0.016*\"color\" + 0.015*\"sleep\" + 0.015*\"hot\"\n",
      "Topic: 11 Word: 0.038*\"dark\" + 0.037*\"shadow\" + 0.036*\"dragon\" + 0.028*\"babi\" + 0.027*\"light\" + 0.025*\"vampir\" + 0.018*\"danc\" + 0.017*\"war\" + 0.016*\"warrior\" + 0.016*\"gift\"\n",
      "Topic: 12 Word: 0.056*\"christma\" + 0.032*\"el\" + 0.031*\"kiss\" + 0.024*\"princess\" + 0.023*\"sin\" + 0.022*\"hunter\" + 0.017*\"detect\" + 0.016*\"golden\" + 0.015*\"novella\" + 0.014*\"face\"\n",
      "Topic: 13 Word: 0.023*\"queen\" + 0.021*\"legend\" + 0.018*\"danger\" + 0.016*\"wife\" + 0.013*\"fli\" + 0.013*\"rain\" + 0.012*\"london\" + 0.012*\"futur\" + 0.011*\"languag\" + 0.011*\"stand\"\n",
      "Topic: 14 Word: 0.037*\"le\" + 0.029*\"dog\" + 0.024*\"bride\" + 0.023*\"witch\" + 0.021*\"perfect\" + 0.021*\"diari\" + 0.019*\"place\" + 0.017*\"desir\" + 0.016*\"promis\" + 0.015*\"seven\"\n",
      "Topic: 15 Word: 0.021*\"school\" + 0.021*\"run\" + 0.019*\"chang\" + 0.018*\"step\" + 0.018*\"happi\" + 0.017*\"green\" + 0.017*\"midnight\" + 0.016*\"girl\" + 0.015*\"line\" + 0.015*\"countri\"\n",
      "Topic: 16 Word: 0.086*\"la\" + 0.032*\"del\" + 0.029*\"thing\" + 0.014*\"revolut\" + 0.013*\"final\" + 0.012*\"saint\" + 0.012*\"el\" + 0.012*\"horror\" + 0.011*\"fate\" + 0.011*\"happen\"\n",
      "Topic: 17 Word: 0.035*\"ghost\" + 0.031*\"beauti\" + 0.029*\"di\" + 0.029*\"في\" + 0.022*\"street\" + 0.020*\"knight\" + 0.017*\"snow\" + 0.017*\"lover\" + 0.016*\"door\" + 0.014*\"call\"\n",
      "Topic: 18 Word: 0.023*\"hero\" + 0.020*\"storm\" + 0.016*\"tell\" + 0.016*\"get\" + 0.016*\"think\" + 0.015*\"wed\" + 0.014*\"essay\" + 0.013*\"da\" + 0.013*\"dan\" + 0.012*\"freedom\"\n",
      "Topic: 19 Word: 0.030*\"vol\" + 0.030*\"bear\" + 0.030*\"wild\" + 0.023*\"like\" + 0.022*\"garden\" + 0.021*\"island\" + 0.020*\"tale\" + 0.020*\"princ\" + 0.019*\"season\" + 0.016*\"fairi\"\n",
      "Topic: 20 Word: 0.040*\"star\" + 0.030*\"break\" + 0.023*\"devil\" + 0.022*\"want\" + 0.019*\"box\" + 0.019*\"set\" + 0.015*\"case\" + 0.015*\"haunt\" + 0.013*\"seri\" + 0.013*\"train\"\n",
      "Topic: 21 Word: 0.039*\"dream\" + 0.027*\"good\" + 0.021*\"lie\" + 0.020*\"stone\" + 0.019*\"land\" + 0.016*\"monster\" + 0.016*\"life\" + 0.015*\"passion\" + 0.014*\"justic\" + 0.013*\"forget\"\n",
      "Topic: 22 Word: 0.043*\"hous\" + 0.026*\"way\" + 0.025*\"white\" + 0.023*\"forev\" + 0.023*\"woman\" + 0.021*\"second\" + 0.020*\"winter\" + 0.018*\"find\" + 0.017*\"chanc\" + 0.016*\"sex\"\n",
      "Topic: 23 Word: 0.035*\"rise\" + 0.033*\"king\" + 0.023*\"billionair\" + 0.022*\"bad\" + 0.021*\"song\" + 0.020*\"master\" + 0.019*\"word\" + 0.019*\"truth\" + 0.019*\"river\" + 0.018*\"hell\"\n",
      "Topic: 24 Word: 0.029*\"moon\" + 0.026*\"end\" + 0.024*\"memoir\" + 0.019*\"children\" + 0.016*\"mother\" + 0.015*\"space\" + 0.013*\"на\" + 0.013*\"hand\" + 0.012*\"deep\" + 0.012*\"money\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance evaluation by classifying sample document using LDA TF-IDF model\n",
      "10\n",
      "\n",
      "Score: 0.5736227035522461\t \n",
      "Topic: 0.035*\"boy\" + 0.030*\"know\" + 0.028*\"summer\" + 0.023*\"doctor\" + 0.021*\"tree\" + 0.017*\"need\" + 0.016*\"troubl\" + 0.016*\"color\" + 0.015*\"sleep\" + 0.015*\"hot\"\n",
      "16\n",
      "\n",
      "Score: 0.1158323809504509\t \n",
      "Topic: 0.086*\"la\" + 0.032*\"del\" + 0.029*\"thing\" + 0.014*\"revolut\" + 0.013*\"final\" + 0.012*\"saint\" + 0.012*\"el\" + 0.012*\"horror\" + 0.011*\"fate\" + 0.011*\"happen\"\n",
      "20\n",
      "\n",
      "Score: 0.11341923475265503\t \n",
      "Topic: 0.040*\"star\" + 0.030*\"break\" + 0.023*\"devil\" + 0.022*\"want\" + 0.019*\"box\" + 0.019*\"set\" + 0.015*\"case\" + 0.015*\"haunt\" + 0.013*\"seri\" + 0.013*\"train\"\n",
      "19\n",
      "\n",
      "Score: 0.11310765892267227\t \n",
      "Topic: 0.030*\"vol\" + 0.030*\"bear\" + 0.030*\"wild\" + 0.023*\"like\" + 0.022*\"garden\" + 0.021*\"island\" + 0.020*\"tale\" + 0.020*\"princ\" + 0.019*\"season\" + 0.016*\"fairi\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance evaluation by classifying sample document using LDA TF-IDF model\")\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4130]], key=lambda tup: -1*tup[1]):\n",
    "    print(index)\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model on unseen document\n",
      "[(0, 0.013333629), (1, 0.013333629), (2, 0.346664), (3, 0.013333629), (4, 0.013333629), (5, 0.013333629), (6, 0.013333629), (7, 0.013333629), (8, 0.013333629), (9, 0.013333629), (10, 0.013333629), (11, 0.013333629), (12, 0.013333629), (13, 0.013333629), (14, 0.013333629), (15, 0.013333629), (16, 0.013333629), (17, 0.013333629), (18, 0.3466625), (19, 0.013333629), (20, 0.013333629), (21, 0.013333629), (22, 0.013333629), (23, 0.013333629), (24, 0.013333629)]\n",
      "________________________________________________________________________________________________\n",
      "2\n",
      "Score: 0.3466639816761017\t Topic: 0.245*\"book\" + 0.087*\"seri\" + 0.022*\"recip\" + 0.021*\"kid\" + 0.020*\"age\" + 0.017*\"queen\" + 0.016*\"child\" + 0.016*\"free\" + 0.012*\"alpha\" + 0.012*\"easi\"\n",
      "18\n",
      "Score: 0.3466625511646271\t Topic: 0.069*\"live\" + 0.045*\"women\" + 0.038*\"work\" + 0.035*\"woman\" + 0.028*\"step\" + 0.026*\"devil\" + 0.026*\"truth\" + 0.024*\"letter\" + 0.024*\"second\" + 0.022*\"classic\"\n",
      "0\n",
      "Score: 0.013333628885447979\t Topic: 0.126*\"world\" + 0.069*\"histori\" + 0.049*\"year\" + 0.025*\"ii\" + 0.024*\"faith\" + 0.023*\"centuri\" + 0.022*\"war\" + 0.019*\"new\" + 0.018*\"ancient\" + 0.018*\"simpl\"\n",
      "1\n",
      "Score: 0.013333628885447979\t Topic: 0.131*\"la\" + 0.060*\"el\" + 0.040*\"del\" + 0.039*\"le\" + 0.030*\"come\" + 0.026*\"wild\" + 0.025*\"break\" + 0.024*\"los\" + 0.019*\"daughter\" + 0.016*\"princess\"\n",
      "3\n",
      "Score: 0.013333628885447979\t Topic: 0.100*\"time\" + 0.057*\"way\" + 0.028*\"garden\" + 0.026*\"stone\" + 0.025*\"kill\" + 0.024*\"old\" + 0.020*\"long\" + 0.020*\"seven\" + 0.017*\"cook\" + 0.017*\"troubl\"\n",
      "4\n",
      "Score: 0.013333628885447979\t Topic: 0.076*\"day\" + 0.055*\"black\" + 0.042*\"citi\" + 0.032*\"guid\" + 0.031*\"end\" + 0.027*\"white\" + 0.022*\"human\" + 0.019*\"build\" + 0.018*\"handbook\" + 0.017*\"color\"\n",
      "5\n",
      "Score: 0.013333628885447979\t Topic: 0.069*\"american\" + 0.052*\"rise\" + 0.046*\"king\" + 0.037*\"saga\" + 0.034*\"soul\" + 0.027*\"summer\" + 0.026*\"land\" + 0.019*\"passion\" + 0.018*\"state\" + 0.015*\"deep\"\n",
      "6\n",
      "Score: 0.013333628885447979\t Topic: 0.042*\"power\" + 0.036*\"thing\" + 0.030*\"les\" + 0.028*\"practic\" + 0.026*\"sweet\" + 0.025*\"anim\" + 0.024*\"word\" + 0.020*\"das\" + 0.020*\"save\" + 0.019*\"self\"\n",
      "7\n",
      "Score: 0.013333628885447979\t Topic: 0.051*\"dead\" + 0.042*\"journey\" + 0.038*\"complet\" + 0.028*\"make\" + 0.023*\"read\" + 0.021*\"surviv\" + 0.020*\"bodi\" + 0.019*\"polit\" + 0.018*\"teach\" + 0.018*\"van\"\n",
      "8\n",
      "Score: 0.013333628885447979\t Topic: 0.052*\"war\" + 0.043*\"christma\" + 0.040*\"boy\" + 0.037*\"dream\" + 0.035*\"dragon\" + 0.034*\"littl\" + 0.032*\"angel\" + 0.031*\"game\" + 0.026*\"scienc\" + 0.025*\"cat\"\n",
      "9\n",
      "Score: 0.013333628885447979\t Topic: 0.082*\"man\" + 0.078*\"girl\" + 0.047*\"home\" + 0.027*\"life\" + 0.027*\"go\" + 0.024*\"bibl\" + 0.022*\"find\" + 0.022*\"studi\" + 0.020*\"real\" + 0.020*\"guid\"\n",
      "10\n",
      "Score: 0.013333628885447979\t Topic: 0.040*\"edit\" + 0.034*\"light\" + 0.030*\"kiss\" + 0.029*\"earth\" + 0.029*\"wolf\" + 0.028*\"club\" + 0.023*\"hell\" + 0.020*\"travel\" + 0.019*\"mountain\" + 0.019*\"west\"\n",
      "11\n",
      "Score: 0.013333628885447979\t Topic: 0.097*\"vol\" + 0.075*\"novel\" + 0.046*\"fall\" + 0.046*\"star\" + 0.044*\"collect\" + 0.038*\"best\" + 0.030*\"blue\" + 0.027*\"friend\" + 0.022*\"master\" + 0.019*\"song\"\n",
      "12\n",
      "Score: 0.013333628885447979\t Topic: 0.066*\"night\" + 0.055*\"death\" + 0.045*\"blood\" + 0.043*\"true\" + 0.037*\"stori\" + 0.029*\"memoir\" + 0.021*\"legend\" + 0.020*\"forev\" + 0.019*\"hunter\" + 0.018*\"mother\"\n",
      "13\n",
      "Score: 0.013333628885447979\t Topic: 0.051*\"trilog\" + 0.036*\"bear\" + 0.034*\"vampir\" + 0.031*\"america\" + 0.023*\"doctor\" + 0.020*\"princ\" + 0.020*\"du\" + 0.019*\"warrior\" + 0.017*\"univers\" + 0.017*\"heal\"\n",
      "14\n",
      "Score: 0.013333628885447979\t Topic: 0.065*\"die\" + 0.059*\"art\" + 0.048*\"der\" + 0.041*\"men\" + 0.034*\"ladi\" + 0.023*\"green\" + 0.022*\"spirit\" + 0.022*\"john\" + 0.021*\"river\" + 0.019*\"texa\"\n",
      "15\n",
      "Score: 0.013333628885447979\t Topic: 0.061*\"adventur\" + 0.036*\"good\" + 0.034*\"chang\" + 0.034*\"moon\" + 0.029*\"play\" + 0.022*\"get\" + 0.021*\"think\" + 0.020*\"case\" + 0.019*\"crime\" + 0.015*\"midnight\"\n",
      "16\n",
      "Score: 0.013333628885447979\t Topic: 0.097*\"mysteri\" + 0.082*\"secret\" + 0.049*\"di\" + 0.046*\"murder\" + 0.040*\"il\" + 0.027*\"school\" + 0.023*\"set\" + 0.021*\"box\" + 0.018*\"lord\" + 0.016*\"justic\"\n",
      "17\n",
      "Score: 0.013333628885447979\t Topic: 0.062*\"famili\" + 0.053*\"great\" + 0.033*\"poem\" + 0.030*\"like\" + 0.027*\"road\" + 0.025*\"design\" + 0.021*\"danc\" + 0.019*\"look\" + 0.018*\"inspir\" + 0.018*\"jack\"\n",
      "19\n",
      "Score: 0.013333628885447979\t Topic: 0.042*\"write\" + 0.041*\"shadow\" + 0.039*\"en\" + 0.034*\"romanc\" + 0.033*\"red\" + 0.032*\"know\" + 0.029*\"mind\" + 0.025*\"busi\" + 0.025*\"diari\" + 0.022*\"modern\"\n",
      "20\n",
      "Score: 0.013333628885447979\t Topic: 0.107*\"volum\" + 0.098*\"heart\" + 0.040*\"dog\" + 0.036*\"beauti\" + 0.031*\"high\" + 0.029*\"season\" + 0.020*\"cookbook\" + 0.019*\"cross\" + 0.017*\"hand\" + 0.017*\"edg\"\n",
      "21\n",
      "Score: 0.013333628885447979\t Topic: 0.233*\"love\" + 0.059*\"chronicl\" + 0.046*\"magic\" + 0.029*\"witch\" + 0.027*\"eye\" + 0.020*\"tell\" + 0.019*\"winter\" + 0.019*\"water\" + 0.017*\"line\" + 0.017*\"essay\"\n",
      "22\n",
      "Score: 0.013333628885447979\t Topic: 0.044*\"bride\" + 0.028*\"lie\" + 0.027*\"miss\" + 0.026*\"happi\" + 0.024*\"place\" + 0.024*\"sister\" + 0.021*\"knight\" + 0.021*\"english\" + 0.019*\"origin\" + 0.018*\"futur\"\n",
      "23\n",
      "Score: 0.013333628885447979\t Topic: 0.175*\"stori\" + 0.058*\"god\" + 0.041*\"hous\" + 0.036*\"short\" + 0.030*\"babi\" + 0.029*\"big\" + 0.027*\"peopl\" + 0.023*\"في\" + 0.019*\"sex\" + 0.018*\"run\"\n",
      "24\n",
      "Score: 0.013333628885447979\t Topic: 0.097*\"tale\" + 0.084*\"dark\" + 0.050*\"lose\" + 0.036*\"children\" + 0.032*\"ghost\" + 0.028*\"learn\" + 0.025*\"natur\" + 0.024*\"cultur\" + 0.023*\"hero\" + 0.021*\"insid\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing model on unseen document\")\n",
    "unseen_document = \"The Free Voice\"\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "print(lda_model[bow_vector])\n",
    "print(\"________________________________________________________________________________________________\")\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(index)\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model on unseen document\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LdaMulticore' object has no attribute 'negative'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1247\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ns_exponent'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1248\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LdaMulticore' object has no attribute 'vocabulary'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-7cf1ce41773a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"trained_model/lda_train_bow.model\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# lda_model = pickle.load(open(filename, 'rb'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mlda_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0munseen_document\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"The Free Voice\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mbow_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munseen_document\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1339\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_old_word2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1341\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_old_word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\deprecated\\word2vec.py\u001b[0m in \u001b[0;36mload_old_word2vec\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_old_word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m     \u001b[0mold_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m     \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vector_size'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     params = {\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\deprecated\\word2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'table'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m             \u001b[0mdelattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'table'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# discard in favor of cum_table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1645\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnegative\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'index2word'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_cum_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# rebuild cum_table from vocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'corpus_count'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LdaMulticore' object has no attribute 'negative'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "print(\"Testing model on unseen document\")\n",
    "filename = \"trained_model/lda_train_bow.model\"\n",
    "# lda_model = pickle.load(open(filename, 'rb'))\n",
    "lda_model = Word2Vec.load(filename)\n",
    "unseen_document = \"The Free Voice\"\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "print(lda_model[bow_vector])\n",
    "print(\"________________________________________________________________________________________________\")\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(index)\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Salman's bag\"\n",
    "a.replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
